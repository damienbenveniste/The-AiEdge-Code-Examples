{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "554d461f",
   "metadata": {},
   "source": [
    "# Deploying with vLLM\n",
    "\n",
    "In this demo, we assume deploying on a simple AWS EC2 instance for simplicity. Beforehand, make sure you have access to enough vCPU on G and VT machine:\n",
    "- [Service Quota](https://us-west-1.console.aws.amazon.com/servicequotas/home/services/ec2/quotas): \n",
    "Running On-Demand G and VT instances\n",
    "\n",
    "In the EC2 dashboard, launch instance (e.g. `g4dn.xlarge`) with an AMI with NVIDIA drivers (e.g `Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.7 (Amazon Linux 2023)`). Select or create a key-pair when launching the instance. You can ssh into the machine:\n",
    "```bash\n",
    "ssh -i vllm-keypair.pem ec2-user@ec2-54-153-9-44.us-west-1.compute.amazonaws.com\n",
    "```\n",
    "\n",
    "If you selected the `Deep Learning OSS Nvidia Driver AMI GPU PyTorch 2.7 (Amazon Linux 2023)` AMI, you can activate the virtual environment:\n",
    "```bash\n",
    "source /opt/pytorch/bin/activate\n",
    "```\n",
    "And you can then install vLLM:\n",
    "```bash\n",
    "pip install vllm\n",
    "```\n",
    "Some Hugging Face model will require a read access token:\n",
    "```bash\n",
    "export HF_TOKEN=[YOUR TOKEN]\n",
    "```\n",
    "\n",
    "Now, you can serve a model:\n",
    "```bash\n",
    "vllm serve meta-llama/Llama-3.2-1B-Instruct --gpu-memory-utilization 0.9 --max-model-len 200\n",
    "```\n",
    "\n",
    "## The OpenAI Client\n",
    "\n",
    "vLLM provides an HTTP server that implements OpenAI's Completions API, so on the client side, we need to install OpenAI:\n",
    "```bash\n",
    "pip install openai\n",
    "```\n",
    "Make sure to open the port 8000 on the EC2 instance. We can now instantiate a client\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d8aafc86",
   "metadata": {},
   "outputs": [],
   "source": [
    "from openai import OpenAI\n",
    "\n",
    "client = OpenAI(\n",
    "    base_url=\"http://ec2-54-153-9-44.us-west-1.compute.amazonaws.com:8000/v1\",\n",
    "    api_key=\"none\",\n",
    ")\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    messages=[\n",
    "    {\"role\": \"user\", \"content\": \"How are you?\"}\n",
    "  ]\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d360b6",
   "metadata": {},
   "source": [
    "We can pass arguments like the temperature, the number output sequence penalty or beam search. \n",
    "- A few of the chat completion API are supported: https://platform.openai.com/docs/api-reference/chat/create\n",
    "- And additional parameters are also supported: https://docs.vllm.ai/en/v0.4.0.post1/serving/openai_compatible_server.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62249ef8",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Which one is greater, 9.11 or 9.8?\"},\n",
    "    ],\n",
    "    extra_body={'use_beam_search': True},\n",
    "    temperature=0.7\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-1B-Instruct\",\n",
    "    messages=[\n",
    "    {\"role\": \"user\", \"content\": \"Which one is greater, 9.11 or 9.8?\"},\n",
    "  ],\n",
    "  n=1,\n",
    "  frequency_penalty=2\n",
    ")\n",
    "\n",
    "print(response.choices[0].message)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ededc4",
   "metadata": {},
   "source": [
    "You can also enable streaming output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a42d114",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"2 + 2=?. Give me your reasoning!\"}\n",
    "    ],\n",
    "    stream=True,\n",
    ")\n",
    "\n",
    "for chunk in response:\n",
    "    if chunk.choices[0].delta.content:\n",
    "        print(chunk.choices[0].delta.content, end=\"\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d06bf3f4",
   "metadata": {},
   "source": [
    "## Tool\n",
    "\n",
    "We can enable the tool parsing argument:\n",
    "```bash\n",
    "vllm serve meta-llama/Llama-3.2-3B-Instruct \\\n",
    "    --guided-decoding-backend outlines \\\n",
    "    --gpu-memory-utilization 0.9 \\\n",
    "    --max-model-len 4000 \\\n",
    "    --enable-auto-tool-choice \\\n",
    "    --tool-call-parser llama3_json\n",
    "```\n",
    "\n",
    "For example, let's use Tavily as a tool"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6def5784",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "from pydantic import BaseModel, Field\n",
    "import json\n",
    "\n",
    "\n",
    "tavily = TavilyClient(api_key='[YOUR TAVILY KEY]')\n",
    "\n",
    "\n",
    "# We define the tool as a function\n",
    "def run_tavily(query: str, max_results: int = 5):\n",
    "    \"\"\"\n",
    "    Executes a web search and returns a JSON string the model can read.\n",
    "    \"\"\"\n",
    "    results = tavily.search(\n",
    "        query=query,\n",
    "        max_results=max_results,\n",
    "        include_answer=False,             # raw results are usually best for an LLM\n",
    "    )\n",
    "    # The open-source models like compact JSON without Python objects\n",
    "    return json.dumps(results)\n",
    "\n",
    "\n",
    "# We can use Pydantic to correctly define the arguments to the tools\n",
    "class SearchFormat(BaseModel):\n",
    "    query: str = Field(\n",
    "        ..., description=\"The search query (what the user wants to know).\"\n",
    "    )\n",
    "    max_results: int = Field(\n",
    "        ..., description=\"How many URLs to return (1-10).\"\n",
    "    )\n",
    "\n",
    "SearchFormat.model_json_schema()['properties']\n",
    "\n",
    "\n",
    "tools = [\n",
    "    {\n",
    "        \"type\": \"function\",\n",
    "        \"function\": {\n",
    "            \"name\": \"tavily_search\",\n",
    "            \"description\": \"Search the web with Tavily and return the top results.\",\n",
    "            \"parameters\": {\n",
    "                \"type\": \"object\",\n",
    "                \"properties\": SearchFormat.model_json_schema()['properties'],\n",
    "                \"required\": [\"query\"]\n",
    "            },\n",
    "        },\n",
    "    }\n",
    "]\n",
    "\n",
    "\n",
    "messages = [\n",
    "    {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
    "    {\"role\": \"user\", \"content\": \"Who won the 2024 Turing Award and why?\"},\n",
    "]\n",
    "\n",
    "while True:\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "        messages=messages,\n",
    "        tools=tools,\n",
    "        tool_choice=\"auto\",               # let the model decide if it needs search\n",
    "    )\n",
    "\n",
    "    msg = response.choices[0].message\n",
    "    tool_calls = msg.tool_calls\n",
    "\n",
    "    # If the model wants to use Tavily, satisfy the request and continue\n",
    "    if tool_calls:\n",
    "        messages.append(\n",
    "            {\n",
    "                \"role\": \"assistant\",\n",
    "                \"tool_calls\": tool_calls,\n",
    "            }\n",
    "        )\n",
    "        for call in tool_calls:\n",
    "            if call.function.name == \"tavily_search\":\n",
    "                args = json.loads(call.function.arguments)\n",
    "                tool_output = run_tavily(args)\n",
    "\n",
    "                messages.append(\n",
    "                    {\n",
    "                        \"role\": \"tool\",\n",
    "                        \"name\": call.function.name,\n",
    "                        \"tool_call_id\": call.id,\n",
    "                        \"content\": tool_output,\n",
    "                    }\n",
    "                )\n",
    "                break\n",
    "        continue               # call the LLM again with new context\n",
    "\n",
    "    # otherwise, the model is done â€“ print its answer\n",
    "    print(msg.content)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36ea0864",
   "metadata": {},
   "source": [
    "## Structured Output\n",
    "\n",
    "We can impose a specific output format "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7e81939",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel\n",
    "\n",
    "\n",
    "class Reasoning(BaseModel):\n",
    "    steps: list[str] = Field(..., description=\"The reasoning steps to answer to the question\")\n",
    "    answer: str\n",
    "    confidence: float = Field(..., gte=0, lte=1)\n",
    "\n",
    "response_format = {\n",
    "    \"type\": \"json_schema\",\n",
    "    \"json_schema\": {\n",
    "        \"name\": \"reasoning\",\n",
    "        \"schema\": Reasoning.model_json_schema(),\n",
    "        # 'strict': True\n",
    "    },\n",
    "}\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"what is 235 + 526\"},\n",
    "    ],\n",
    "    response_format=response_format\n",
    ")\n",
    "\n",
    "print(response.choices[0])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6f199ad",
   "metadata": {},
   "source": [
    "Or by using the `parse` API:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd42fafa",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.parse(\n",
    "    model=\"meta-llama/Llama-3.2-3B-Instruct\",\n",
    "    messages=[\n",
    "         {\"role\": \"user\", \"content\": \"what is 235 + 526\"},\n",
    "    ],\n",
    "    response_format=Reasoning\n",
    ")\n",
    "\n",
    "print(response.choices[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55e517bf",
   "metadata": {},
   "source": [
    "## Reasoning Models\n",
    "\n",
    "With reasoning models, we can isolate the reasoning:\n",
    "```bash\n",
    "vllm serve deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B --reasoning-parser deepseek_r1 --gpu-memory-utilization 0.9 --max-model-len 4000\n",
    "````\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d91cb76e",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "965f2e37",
   "metadata": {},
   "outputs": [],
   "source": [
    "response = client.chat.completions.create(\n",
    "    model=\"deepseek-ai/DeepSeek-R1-Distill-Qwen-1.5B\",\n",
    "    messages=[\n",
    "        {\"role\": \"user\", \"content\": \"Which one is greater, 9.11 or 9.8?\"},\n",
    "    ],\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.reasoning_content)\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c004c0a",
   "metadata": {},
   "source": [
    "## Multimodal Models\n",
    "\n",
    "VLLM supports multimodal models:\n",
    "```bash\n",
    "vllm serve microsoft/Phi-3.5-vision-instruct --trust-remote-code --max-model-len 4096 --limit-mm-per-prompt '{\"image\":2}' --gpu-memory-utilization 0.9\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d686dc0",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_url = \"https://upload.wikimedia.org/wikipedia/commons/thumb/d/dd/Gfp-wisconsin-madison-the-nature-boardwalk.jpg/2560px-Gfp-wisconsin-madison-the-nature-boardwalk.jpg\"\n",
    "\n",
    "messages = [\n",
    "    {\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\"url\": image_url},\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "]\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    model=\"microsoft/Phi-3.5-vision-instruct\",\n",
    "    messages=messages,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d121e517",
   "metadata": {},
   "source": [
    "Or from a local image:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dea4d",
   "metadata": {},
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import base64\n",
    "\n",
    "img = Image(filename=\"Transformers-One.png\")\n",
    "data_uri = f\"data:image/png;base64,{base64.b64encode(img.data).decode('utf-8')}\"\n",
    "\n",
    "response = client.chat.completions.create(\n",
    "    messages=[{\n",
    "        \"role\": \"user\",\n",
    "        \"content\": [\n",
    "            {\"type\": \"text\", \"text\": \"What's in this image?\"},\n",
    "            {\"type\": \"image_url\", \"image_url\": {\"url\": data_uri}},\n",
    "        ],\n",
    "    }],\n",
    "    model=\"microsoft/Phi-3.5-vision-instruct\",\n",
    "    # max_completion_tokens=64,\n",
    ")\n",
    "\n",
    "print(response.choices[0].message.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
