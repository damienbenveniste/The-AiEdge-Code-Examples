{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8cf915ae",
   "metadata": {},
   "source": [
    "# Training LLMs: Complete Pipeline from Pretraining to RLHF\n",
    "\n",
    "This notebook demonstrates the full training pipeline for large language models through three main stages:\n",
    "\n",
    "1. **Pretraining** - Training a model from scratch on raw text\n",
    "2. **Supervised Fine-tuning (SFT)** - Teaching instruction following  \n",
    "3. **Reinforcement Learning from Human Feedback (RLHF)** - Aligning with human preferences\n",
    "\n",
    "We'll build a small Mistral model and train it end-to-end to understand each stage."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff222205",
   "metadata": {},
   "source": [
    "# Stage 1: Pretraining\n",
    "\n",
    "In this section, we'll train a model from scratch using next-token prediction on raw Wikipedia text. This teaches the model basic language understanding and generation capabilities."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fee02162",
   "metadata": {},
   "source": [
    "## Data Loading\n",
    "\n",
    "Here we're downloading a small subset (1000 samples) of the Wikipedia dataset for demonstration. This gives us raw text data that we'll use for pretraining our model from scratch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ffb2f095",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "317b5b78b84842d491dd3a017193a7d7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Resolving data files:   0%|          | 0/41 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "\n",
    "wiki_data = load_dataset(\n",
    "    \"wikimedia/wikipedia\",   \n",
    "    \"20231101.en\", \n",
    "    split=\"train[:1000]\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c710519e",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Anarchism is a political philosophy and movement that is skeptical of all justifications for authority and seeks to abolish the institutions it claims maintain unnecessary coercion and hierarchy, typically including nation-states, and capitalism. Anarchism advocates for the replacement of the state with stateless societies and voluntary free associations. As a historically left-wing movement, this reading of anarchism is placed on the farthest left of the political spectrum, usually described as the libertarian wing of the socialist movement (libertarian socialism).\n",
      "\n",
      "Humans have lived in societies without formal hierarchies long before the establishment of states, realms, or empires. With the rise of organised hierarchical bodies, scepticism toward authority also rose. Although traces of anarchist ideas are found all throughout history, modern anarchism emerged from the Enlightenment. During the latter half of the 19th and the first decades of the 20th century, the anarchist movement f\n"
     ]
    }
   ],
   "source": [
    "print(wiki_data['text'][0][:1000])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e48d1b8",
   "metadata": {},
   "source": [
    "Let's split the data into train and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "acf23413",
   "metadata": {},
   "outputs": [],
   "source": [
    "wiki_data = wiki_data.train_test_split(test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "6ce79e42",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 800\n",
       "    })\n",
       "    test: Dataset({\n",
       "        features: ['id', 'url', 'title', 'text'],\n",
       "        num_rows: 200\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wiki_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "354fdc29",
   "metadata": {},
   "source": [
    "## Tokenization Process\n",
    "\n",
    "This shows what tokenization looks like - converting raw text into numerical tokens that the model can process."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45c686c7",
   "metadata": {},
   "source": [
    "I am going to train a model from scratch I am going to use the Mistral architecture as base. I will use the same tokenize to move from text data to the input index data. I had to run `huggingface-cli login` and enter my token to download this specific model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "2d8d737e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoTokenizer\n",
    "\n",
    "base_model_id = 'mistralai/Mistral-7B-v0.1'\n",
    "tokenizer = AutoTokenizer.from_pretrained(base_model_id)\n",
    "tokenizer.pad_token = tokenizer.eos_token"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "407c26dc",
   "metadata": {},
   "source": [
    "The padding token was missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0b4d65d5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bos_token': '<s>',\n",
       " 'eos_token': '</s>',\n",
       " 'unk_token': '<unk>',\n",
       " 'pad_token': '</s>'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.special_tokens_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ea0bdf",
   "metadata": {},
   "source": [
    "Tokenizing the data is easy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "674133a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "outputs = tokenizer(\n",
    "    wiki_data['train']['text'][0:10],\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15939709",
   "metadata": {},
   "source": [
    "This function processes our text data in batches:\n",
    "- `truncation=True, max_length=512`: Cuts off text longer than 512 tokens\n",
    "- `padding='max_length'`: Pads shorter sequences to 512 tokens with pad tokens\n",
    "- `return_tensors=\"pt\"`: Returns PyTorch tensors instead of lists\n",
    "- `remove_columns`: Removes original text columns, keeping only tokenized data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a5ddaa71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "38330a03c7bf42398bf987d11b494e31",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/800 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c95316c93e7b4dc1827a5117f9e38864",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "max_length = 512\n",
    "\n",
    "def tokenize_function(examples):\n",
    "    return tokenizer(\n",
    "        examples['text'], \n",
    "        truncation=True, \n",
    "        max_length=max_length, \n",
    "        padding='max_length', # longuest \n",
    "        return_tensors=\"pt\", \n",
    "        add_special_tokens=True\n",
    "    )\n",
    "\n",
    "tokenized_datasets = wiki_data.map(\n",
    "    tokenize_function, \n",
    "    batched=True, \n",
    "    remove_columns=['id', 'url', 'title', 'text']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c00e0cb8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'left'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.padding_side"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b3550acf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2523a0a",
   "metadata": {},
   "source": [
    "Notice that `padding_side='left'` means padding tokens are added to the beginning of sequences, and `pad_token_id=2` shows padding uses the same token as EOS (end of sequence)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e534fa36",
   "metadata": {},
   "source": [
    "## Model Architecture\n",
    "\n",
    "First we load the default Mistral configuration to see the full-size model parameters. This shows a 7B parameter model with 32 layers, 4096 hidden dimensions, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "03c42751",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import MistralForCausalLM, MistralConfig\n",
    "config = MistralConfig()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5ce668ef",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralConfig {\n",
       "  \"attention_dropout\": 0.0,\n",
       "  \"bos_token_id\": 1,\n",
       "  \"eos_token_id\": 2,\n",
       "  \"head_dim\": null,\n",
       "  \"hidden_act\": \"silu\",\n",
       "  \"hidden_size\": 4096,\n",
       "  \"initializer_range\": 0.02,\n",
       "  \"intermediate_size\": 14336,\n",
       "  \"max_position_embeddings\": 131072,\n",
       "  \"model_type\": \"mistral\",\n",
       "  \"num_attention_heads\": 32,\n",
       "  \"num_hidden_layers\": 32,\n",
       "  \"num_key_value_heads\": 8,\n",
       "  \"rms_norm_eps\": 1e-06,\n",
       "  \"rope_theta\": 10000.0,\n",
       "  \"sliding_window\": 4096,\n",
       "  \"tie_word_embeddings\": false,\n",
       "  \"transformers_version\": \"4.52.4\",\n",
       "  \"use_cache\": true,\n",
       "  \"vocab_size\": 32000\n",
       "}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "477dea5c",
   "metadata": {},
   "source": [
    "Here we can see the default Mistral configuration showing a 7B parameter model with 32 layers, 4096 hidden dimensions, etc. This is too large for our demonstration."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7e3fc73",
   "metadata": {},
   "source": [
    "Now we create a much smaller model configuration for demonstration:\n",
    "- `hidden_size=768`: Reduced from 4096 to make training faster\n",
    "- `num_hidden_layers=4`: Only 4 layers instead of 32\n",
    "- `num_attention_heads=16`: Reduced attention heads\n",
    "- `intermediate_size=3072`: Smaller feed-forward network\n",
    "- `max_position_embeddings=512`: Matches our sequence length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a6a47039",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = MistralConfig(\n",
    "    hidden_size=768,\n",
    "    sliding_window=768,\n",
    "    intermediate_size=3072,\n",
    "    max_position_embeddings=max_length,\n",
    "    num_attention_heads=16,  \n",
    "    num_hidden_layers=4,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1fcd06e",
   "metadata": {},
   "source": [
    "This creates our small Mistral model with the reduced configuration. The model structure shows 4 decoder layers with our specified dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d20326b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MistralForCausalLM(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "cad5a4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForCausalLM(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((768,), eps=1e-06)\n",
       "        (post_attention_layernorm): MistralRMSNorm((768,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((768,), eps=1e-06)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=768, out_features=32000, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8177ff80",
   "metadata": {},
   "source": [
    "Our small model has ~84M parameters compared to the 7B in the original Mistral model. This makes it feasible to train on modest hardware."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "0e9fa6a9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "84548352"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_size = sum(t.numel() for t in model.parameters())\n",
    "model_size"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31264846",
   "metadata": {},
   "source": [
    "## Data Collation for Language Modeling\n",
    "\n",
    "`DataCollatorForLanguageModeling` with `mlm=False` sets up causal language modeling (predicting next tokens). It automatically creates labels by shifting input_ids by one position."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "69854059",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import DataCollatorForLanguageModeling\n",
    "\n",
    "data_collator = DataCollatorForLanguageModeling(tokenizer, mlm=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3b301a1",
   "metadata": {},
   "source": [
    "This shows what a batch looks like after collation. We have 10 sequences, each tokenized and ready for training. The data collator creates batches from our tokenized data. Let's see what a batch looks like with 10 sequences:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a2759ccc",
   "metadata": {},
   "outputs": [],
   "source": [
    "out = data_collator([\n",
    "    tokenized_datasets[\"train\"][i] for i in range(10)\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1ab17da",
   "metadata": {},
   "source": [
    "This shows the first sequence's input_ids - the tokenized text ready for training. These are the numerical tokens the model will learn from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "225f32cc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1,  2740,  3255,   987,   968,  2677,   325,  2854, 28731,   349,\n",
       "          264, 28705,   968,  2677, 11108,  1307,   297, 13176,  8520, 28725,\n",
       "         1080, 14473,   354, 27179,  3257,  8570,   395,   264,  6480,  8333,\n",
       "        28723,   560, 24157,   968,  2677, 28725,   272, 24157,   325, 16530,\n",
       "         6342, 28731,   302,   272,  8333,   349, 20331,   297, 19938,   298,\n",
       "          369,   302,   272,  2928,  7528, 28725,  1259,   390,   396, 10466,\n",
       "         7528, 28723,   851, 11108,  9349, 28713,   395, 10417,   968,  2677,\n",
       "        28725,   297,   690,  2477,   272, 11010,   302,   272, 20320,  8333,\n",
       "          349, 20331, 28725,   390,   297, 11010,   968,  2677, 28725,   442,\n",
       "          871,  6896, 28725,   390,   297,  6896,   968,  2677, 28723,    13,\n",
       "           13,  2854,   403,   272, 21864,   968,  2677,  2038,  1307,   354,\n",
       "        27179,  3257, 10466,   297,  6480, 11837,   288, 28723,   661,   403,\n",
       "         6202,  1938,   272,   907,  8249,   302,   272, 28705, 28750, 28734,\n",
       "          362,  5445,  5398,   395, 27996,  5062,   479,   340,   351,   423,\n",
       "        28708,   304,   399,  1278,  3165,   401,   409,  8331, 28742, 28713,\n",
       "        16893,  1590,   291,  6690, 14107,   297, 28705, 28740, 28774, 28734,\n",
       "        28734, 28723,   851,  3493,  1221,   302, 10401,   349,  4662,  1987,\n",
       "         3579, 28733,  2205,  3785, 24157,   968,  2677,   325,  7277, 28760,\n",
       "         2854,   557,  1096,   272,  4787,  2038, 17805,  2081, 25299,   356,\n",
       "         2477,  2081,   302,   272, 20320, 11010, 28723, 16096, 28733,  2205,\n",
       "         3785,   968,  2677,  6098,  4028,  4119, 17123,   298, 20545,   624,\n",
       "          302,   272,  2081, 25299,   304,  8189,   272, 20320,  7528, 28725,\n",
       "          690,  3267,  1855,   272, 10554,   302,  2928,  1982,   298,  3102,\n",
       "        16209,  1982, 28725, 21123,  1982, 12852,  8296,   302,  1407, 13750,\n",
       "          404, 28725,   304,  4069,  1046,  1873,  4028,  2617,  4479,  1837,\n",
       "          302,   272, 16209, 10312, 28723,    13,    13,  2854,  7520,   297,\n",
       "          938,   297,  1287,  6967,   302,  8520,   297,  4518,   298, 10401,\n",
       "        11837,   288, 28747,  2485, 21884,  6480, 28725, 25820,  6480, 28725,\n",
       "          989, 28733,  1128,  2847,  2806, 28725,   550, 28769, 28765, 11475,\n",
       "         6480, 28725,  9893,  4028,  6480, 28725,   304,   297,  6074,   968,\n",
       "         7940,   297,   272,  1221,   302,  1186,  2854, 28723,    13,    13,\n",
       "        23295, 28705,    13,   657, 14044,  1063, 28725,  6584, 15976, 10399,\n",
       "          304, 24367, 28725,   968,  2677,  2825, 22312,   741,  6563,   302,\n",
       "          264, 12734,  8333, 20320,  7528,   395,   396,  1871, 28733, 28726,\n",
       "        12385,   968,  2677,  8333,   674, 28725,  1259,   390,   396, 10466,\n",
       "         7528,   690, 10651,  2622, 28725,   442,   264,  3798,  7528,   690,\n",
       "        10651,  6203, 28723,   560,   456,  3367, 28725,   272, 20320,  8333,\n",
       "        28725,   690,   659,   264,  1188,  4337, 11010,   821,   272,  2928,\n",
       "         7528, 28725, 21277,   272,  1871, 28723,  1794,   272, 11864,  5086,\n",
       "        28725,   272,  2928,  7528,   349, 25081,   477,   272,   968,  6432,\n",
       "        20320,   486,   340,  1398,  2677, 28723,    13,    13,   657,  2952,\n",
       "         1221, 28725,   264,   968,  2677,  1759,   302,   264,  5450,   381,\n",
       "          806,   282, 20320,  8333,   993,   347,  5397,   486,   272,  2296,\n",
       "         8777, 28747,    13,    13, 28723,    13,    13, 28741, 28732, 28707,\n",
       "        28731, 10651,   272,   727, 28733, 28728,   628,   288, 24157,   302,\n",
       "          272,  5450,   381,   806,   282, 20320,  8333,   304,   272,  6841,\n",
       "          473, 28733,  6590,   349,   272, 20320,   438,   871, 19382, 11010,\n",
       "         1200,   304,   272, 28705, 10990, 18834,  6896,  1847,  6752,   842,\n",
       "          851,  5436,  5090,  5312,   272,   989,  3014,  4938,   302,   968,\n",
       "         2677, 28725, 24157,   968,  2677,   304, 10417,   968,  2677, 28723,\n",
       "          560, 28705, 10417,   968,  2677, 28725,   272,  1850,   330, 28732,\n",
       "        28707, 28731])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['input_ids'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c2bfcde",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([    1, 22742, 18199,  4535,   349,   264,  5567, 12323,  4654,  2071,\n",
       "          477,   650,  8891,  4146, 28725,   264,  6460,   294,  2990,   297,\n",
       "          365,  7547, 28733, 28780,  2355,   781,  1314, 28721, 28725,  3658,\n",
       "         7293, 28723, 22742, 18199,  4535,   403, 11573,   297, 28705, 28740,\n",
       "        28774, 28783, 28787,   486,   320,  7997,   393, 28725,   393,  7473,\n",
       "          392, 28725,  2404, 28706, 28733,  6167, 28725, 18767,  9360, 15834,\n",
       "          325, 28755,  2474,   384,   508,   766, 28731,   304, 16416,  5727,\n",
       "          338, 28723,  7066,  4292,   302,   272,  2071,  8288,  5567,  9893,\n",
       "         1557, 28725,   304,   320,  7997,   393, 28725,   393,  7473,   392,\n",
       "        28725,   304,  5727,   338,   460,   302, 10088, 28725, 15526,  2238,\n",
       "          753, 28725,   304,   382,  1022,   753,  5414, 28713, 28725,  8628,\n",
       "        28723,    13,    13,   657, 17262,  4697,   486,  3964,  2556,  1859,\n",
       "         1929,  9994,  5017,   304,   272, 16024, 12166,  1139,  6249, 28725,\n",
       "        22742, 18199,  4535,   349, 15390,   390,   624,   302,   272,  2191,\n",
       "        21235,   404,   297,  5567, 12323,  4654, 28723,  1306,   654,   624,\n",
       "          302,   272,   907,  4938,   298,  5017,   297,  5567,   325, 27847,\n",
       "          652,  1141,   349,   297,  4300,   609, 13695, 28725,   652,  9184,\n",
       "        13616,  1006, 24720,  2809,   304,  4355,  4382, 28725, 11731,  5596,\n",
       "          706,   477,  2935,  5567, 12323,  4654,  2071,   345, 14892, 19800,\n",
       "          529,  3096,   550,   749, 28739,   325,  1014, 19800,  3953,  9611,\n",
       "          557, 28705,   690,   553,   264,   680,  2061, 28733, 14754,   286,\n",
       "        28725,  1156,  1007, 28725,  4150,  3469, 28723,    13,    13, 28743,\n",
       "          492,   263,    13, 20300,  3692, 18199,  4535, 11220,   408,  3854,\n",
       "          684,   652,  4621,   304,  9021,   390,  2436,   302, 22475, 28725,\n",
       "        10594,   288,   272, 25286,  1837,  8304,   486,  1080, 18433,  7626,\n",
       "         1218,   297,  7293, 28725,   304,   272,  9388,   302, 22802,   304,\n",
       "        26159,   466,   369,  1250, 16703,   264,  5567,  8208,   541,  4244,\n",
       "        28723,   415,  4034,   345, 28765,  1569, 28715,   503, 11060,   269,\n",
       "         5062, 28739,   325, 23314,   297,   574,  1216,  5878, 28731,   403,\n",
       "         5242,   486, 22742, 18199,  4535,   297,  4349, 28705, 28740, 28774,\n",
       "        28774, 28750, 28723,   415,  2692,  3246,   264,   341,   377,   291,\n",
       "          297,   272,  5567, 12323,  4654,  6337, 28723,   661,  1269,   264,\n",
       "         2967,  6251,   684,   272,  3425,   302, 22475,  5473,  7293, 28725,\n",
       "          390,   272,  2071,   403, 15021,   302,  6079, 28733, 28711,  1249,\n",
       "          304,  6079, 28733, 21182,   505,  3338, 28723,   415,  3798,  4370,\n",
       "         2856,  3338,  5804,  5596,   652,  5567,  1455,  2729,   390,   264,\n",
       "         6695,   352,   302,   652,  5567,  9893,  1557,   298, 11749,   447,\n",
       "          745,   304,   521, 12678,   288,   464,   761,  6374, 28742, 18121,\n",
       "        28723,    13,    13,  3260,  3028,   302,  4282,  8208,   349,  2278,\n",
       "        28725,   390,  1287,  5017, 10611,   297,  7293,   506,   750,   302,\n",
       "         7223,  5016, 28723,  2957,   579, 28733, 11565, 28505,   283,  1105,\n",
       "         1685,   325,  2851,   374,  7433, 28731,  2436,  2672,  1721, 28715,\n",
       "          617, 28725, 18216,   496,  8236, 28725,  5017,  3427, 28725,   304,\n",
       "        12323,  4654,  5679,   390,   264,  2825,   302,  4072,   288,  3892,\n",
       "        28723,  4577,   272,  5614,   302,   345, 28765,  1569, 28715,   503,\n",
       "        11060,   269,  5062,   548,  1287,   799,  5567, 28733, 11904, 21779,\n",
       "          404,   506,   835,  3851,   298, 12773,  6891, 28733,  8645,  3421,\n",
       "          440,  5766,   304,  1950, 18978,   302,  9893,  1557, 28723,  2993,\n",
       "        28725,  2070,  1287, 18433, 21118,  9539,   297,  7293,  1300, 28705,\n",
       "         1167,  5567,  8208, 18978, 27168, 28725,  2663,  2204,   272,  8646,\n",
       "          302, 22475,   298,   347,  2598,   390,  5567,  3444,  6308, 28725,\n",
       "          304,   590])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "out['labels'][0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd1473a8",
   "metadata": {},
   "source": [
    "## Training Configuration and Execution\n",
    "\n",
    "Now we set up the training arguments and run the pretraining:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15f19380",
   "metadata": {},
   "source": [
    "The training shows typical pretraining behavior: high initial loss that gradually decreases. The final loss of indicates the model is learning to predict next tokens, though it would need much more training to be truly useful."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "a4e5c2d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damienbenveniste/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='800' max='800' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [800/800 00:45, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>500</td>\n",
       "      <td>6.272200</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=800, training_loss=6.247486724853515, metrics={'train_runtime': 45.461, 'train_samples_per_second': 17.598, 'train_steps_per_second': 17.598, 'total_flos': 147388052275200.0, 'train_loss': 6.247486724853515, 'epoch': 1.0})"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import Trainer, TrainingArguments\n",
    "\n",
    "args = TrainingArguments(\n",
    "    output_dir=\"mistral-pretraining\",\n",
    "    per_device_train_batch_size=1,\n",
    "    per_device_eval_batch_size=1,\n",
    "    num_train_epochs=1,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"none\", \n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    processing_class=tokenizer,\n",
    "    args=args,\n",
    "    data_collator=data_collator,\n",
    "    train_dataset=tokenized_datasets[\"train\"],\n",
    "    eval_dataset=tokenized_datasets[\"test\"],\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3f8f8cd",
   "metadata": {},
   "source": [
    "We need to log into Hugging Face Hub to push our trained models."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "6335e2de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/damienbenveniste/mistral-pretraining/commit/8f4f09a8d1f38dfa60bd43f74b22ce5849f105f4', commit_message='End of training', commit_description='', oid='8f4f09a8d1f38dfa60bd43f74b22ce5849f105f4', pr_url=None, repo_url=RepoUrl('https://huggingface.co/damienbenveniste/mistral-pretraining', endpoint='https://huggingface.co', repo_type='model', repo_id='damienbenveniste/mistral-pretraining'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef9064d0",
   "metadata": {},
   "source": [
    "Let's test our pretrained model with a simple text generation pipeline. Testing our pretrained model shows it generates mostly gibberish - this is expected since it's only been trained for 1 epoch on a tiny dataset. The model hasn't learned coherent language patterns yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "4c7f2d6a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bdbf4188f095407aae3f10e06d0c0868",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/338M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use mps:0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'How are you? (, In a 1918) was the 11194.\\n\\n\\nAor was an American Civillo-1971 was a French of his first part of the German of Aa, a major of the Battle of 1991, the early 1991. He was born in the 127, 1960s, as the American-based-day called the 1800, and the first life.\\n\\nAlc-a of the term, it was born in the first a 1919, he was the most of the 1999.\\n\\nAl-century Men was the S-S.\\n\\nAl-\\n\\n\\nAla was the 1960, the 170, and his name, the first 1900. He was a word he was the 1901, a American first high-B.\\n\\nEarly life\\n\\n\\nThe first British of the 1919, the world of the French of the third of the United States of the 1932.\\n\\nThe 1884 and'"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from transformers import pipeline\n",
    "\n",
    "model_id = \"damienbenveniste/mistral-pretraining\"\n",
    "pipe = pipeline(\"text-generation\", model=model_id)\n",
    "txt = \"How are you?\"\n",
    "results = pipe(txt, num_return_sequences=1)\n",
    "results[0][\"generated_text\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5d214e1",
   "metadata": {},
   "source": [
    "The model generates mostly nonsensical text - this is expected since it's only been trained for 1 epoch on a tiny dataset. It hasn't learned coherent language patterns yet."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4134610",
   "metadata": {},
   "source": [
    "# Stage 2: Supervised Fine-tuning (SFT)\n",
    "\n",
    "Now we'll take our pretrained model and teach it to follow instructions using the Alpaca dataset. This transforms the model from a general text generator into an instruction-following assistant."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62a8e638",
   "metadata": {},
   "source": [
    "We push the trained model to Hugging Face Hub. The commit shows our model is now available as \"damienbenveniste/mistral-pretraining\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "7b2314ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[:1000]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "201b210a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Give three tips for staying healthy.'"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset['instruction'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "a70bd4e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Give three tips for staying healthy.\n",
      "\n",
      "### Response:\n",
      "1.Eat a balanced diet and make sure to include plenty of fruits and vegetables. \n",
      "2. Exercise regularly to keep your body active and strong. \n",
      "3. Get enough sleep and maintain a consistent sleep schedule.\n"
     ]
    }
   ],
   "source": [
    "out = dataset[0]['text']\n",
    "print(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8936d5a8",
   "metadata": {},
   "source": [
    "The formatted text includes prompt structure with \"### Instruction:\" and \"### Response:\" markers. This teaches the model to understand and respond to instructions in this specific format."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e075dd8",
   "metadata": {},
   "source": [
    "We load our pretrained model and tokenizer from the previous step. Notice the model doesn't have a `pad_token_id` set, which we'll need for batching."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53ea25ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_id = \"damienbenveniste/mistral-pretraining\"\n",
    "model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1d3af2",
   "metadata": {},
   "source": [
    "This code sets up a specialized data collator for training language models with instruction-following capabilities:\n",
    "\n",
    "  **`DataCollatorForCompletionOnlyLM`** - A TRL (Transformer Reinforcement Learning) utility that masks the loss computation to only calculate loss on the completion/response portion\n",
    "  of instruction-response pairs, not on the instruction itself.\n",
    "\n",
    "  **`response_template`** - Defines the delimiter `\"\\n### Response:\"` that separates instructions from responses in the training data format.\n",
    "\n",
    "  **`response_template_ids`** - Converts the template to token IDs using the tokenizer, with `[2:]` slicing to remove special tokens (likely BOS/EOS tokens) that aren't part of the \n",
    "  actual template.\n",
    "\n",
    "  **`data_collator`** - Creates the collator instance that will automatically mask instruction tokens during training, ensuring the model only learns to predict response tokens, which\n",
    "   improves instruction-following performance and prevents the model from learning to repeat instructions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "9b5a01e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from trl import DataCollatorForCompletionOnlyLM\n",
    "\n",
    "response_template = \"\\n### Response:\"\n",
    "response_template_ids = tokenizer.encode(response_template, add_special_tokens=False)[2:]\n",
    "\n",
    "data_collator = DataCollatorForCompletionOnlyLM(response_template_ids, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "75534dd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "What are the three primary colors?\n",
      "\n",
      "### Response:\n",
      "The three primary colors are red, blue, and yellow.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "0c9085b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'input_ids': tensor([[    1, 20811,   349,   396, 13126,   369, 13966,   264,  3638, 28723,\n",
       "         12018,   264,  2899,   369,  6582,  1999,  2691,   274,   272,  2159,\n",
       "         28723,    13,    13, 27332,  3133,  3112, 28747,    13,  3195,   460,\n",
       "           272,  1712,  6258,  9304, 28804,    13,    13, 27332, 12107, 28747,\n",
       "            13,  1014,  1712,  6258,  9304,   460,  2760, 28725,  5045, 28725,\n",
       "           304,  9684, 28723]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "         1, 1, 1, 1, 1]]), 'labels': tensor([[ -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "          -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,  -100,\n",
       "            13,  1014,  1712,  6258,  9304,   460,  2760, 28725,  5045, 28725,\n",
       "           304,  9684, 28723]])}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokenized = tokenizer(\n",
    "    dataset['text'][1], \n",
    ")\n",
    "\n",
    "data_collator([tokenized])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a0d8fba",
   "metadata": {},
   "source": [
    "Notice in the labels: `-100` values indicate tokens where loss shouldn't be calculated (the instruction part), while actual token IDs appear only for the response section. This is how we train only on the response generation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a3e5a58",
   "metadata": {},
   "source": [
    "Now we run supervised fine-tuning using SFTTrainer. The training shows a loss of ~6.8, which is lower than pretraining because we're learning a more focused task with structured examples.\n",
    "\n",
    "This code configures and runs supervised fine-tuning using TRL's SFTTrainer:\n",
    "\n",
    "  **`SFTConfig`** - Configuration object that defines training parameters including output directory, dataset field containing text data, sequence length limits, training epochs, and\n",
    "  hub settings.\n",
    "\n",
    "  **`SFTTrainer`** - The main training class that handles supervised fine-tuning with the specified model, dataset, data collator (for completion-only training), and tokenizer as the\n",
    "  processing class.\n",
    "\n",
    "  **`trainer.train()`** - Executes the training loop with the configured parameters, fine-tuning the model on the instruction-response dataset while only computing loss on response\n",
    "  tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "7306bd2d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8cbe6b10d5d74a7d9b95e34a1169afa0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Tokenizing train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b535c00a04444b7e81e35680c6f2edd2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Truncating train dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damienbenveniste/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 01:35, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=6.7392998046875, metrics={'train_runtime': 97.7951, 'train_samples_per_second': 10.225, 'train_steps_per_second': 1.278, 'total_flos': 76175442173952.0, 'train_loss': 6.7392998046875})"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import SFTTrainer, SFTConfig\n",
    "\n",
    "args = SFTConfig(\n",
    "    output_dir=\"mistral-supervised\",\n",
    "    dataset_text_field=\"text\",\n",
    "    max_seq_length=512,\n",
    "    num_train_epochs=1,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"none\", \n",
    ")\n",
    "\n",
    "trainer = SFTTrainer(\n",
    "    model,\n",
    "    args=args,\n",
    "    train_dataset=dataset,\n",
    "    data_collator=data_collator,\n",
    "    processing_class=tokenizer,\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e6a9b6d",
   "metadata": {},
   "source": [
    "And we can push to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "1189cd9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/damienbenveniste/mistral-supervised/commit/97ef8fceeb512a314701234309da2f6d3f1c5ea6', commit_message='End of training', commit_description='', oid='97ef8fceeb512a314701234309da2f6d3f1c5ea6', pr_url=None, repo_url=RepoUrl('https://huggingface.co/damienbenveniste/mistral-supervised', endpoint='https://huggingface.co', repo_type='model', repo_id='damienbenveniste/mistral-supervised'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e635ec30",
   "metadata": {},
   "source": [
    "## RLHF Data Preparation\n",
    "\n",
    "The HH-RLHF dataset contains pairs of responses: one chosen (preferred) and one rejected (less preferred) for the same conversation. This data captures human preferences about response quality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "12fd3238",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"Anthropic/hh-rlhf\", split='train[:1000]')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1c95d5e",
   "metadata": {},
   "source": [
    "This shows a \"chosen\" response that was preferred by human annotators. The dataset contains pairs of chosen vs rejected responses for the same conversation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "239db6bc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      "\n",
      "Assistant: Here’s an incomplete list.\n",
      "\n",
      "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\n",
      "\n",
      "Human: What's your favorite one?\n",
      "\n",
      "Assistant: I haven't even thought about it.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['chosen'][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "a52f760f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "Human: What are some cuss words in english?\n",
      "\n",
      "Assistant: Here’s an incomplete list.\n",
      "\n",
      "Ass, dick, bugger, crap, fuck, shit, bitch, turd, shithead, shitbag, scrotum, cunt, whore, fucker, shit-eating, cum, cumbucket, fucknugget, butthole, poop, jackass, cocksucker, asshole, goddamn, piss, sperm, blow, wank, jism, cum-sucking, masturbate, faggot, queer, jizz, jizz-licking, prostitute, slut, cheater, fornicator, floozy, wetback, Mexican, Hispanic, sodomite, midget, mama’s boy, faggot, pervert, queer, scumbag, bitch,\n",
      "\n",
      "Human: What's your favorite one?\n",
      "\n",
      "Assistant: Ass.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['rejected'][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d825881",
   "metadata": {},
   "source": [
    "We preprocess the data by tokenizing both chosen and rejected responses separately. The reward model will learn to assign higher scores to chosen responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "32ad4c71",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7f16b03cc5644fed8aacc8f590720b35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def preprocess_function(examples):\n",
    "    new_examples = {\n",
    "        \"input_ids_chosen\": [],\n",
    "        \"attention_mask_chosen\": [],\n",
    "        \"input_ids_rejected\": [],\n",
    "        \"attention_mask_rejected\": [],\n",
    "    }\n",
    "    for chosen, rejected in zip(examples[\"chosen\"], examples[\"rejected\"]):\n",
    "        tokenized_chosen = tokenizer(chosen)\n",
    "        tokenized_rejected = tokenizer(rejected)\n",
    "\n",
    "        new_examples[\"input_ids_chosen\"].append(tokenized_chosen[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_chosen\"].append(tokenized_chosen[\"attention_mask\"])\n",
    "        new_examples[\"input_ids_rejected\"].append(tokenized_rejected[\"input_ids\"])\n",
    "        new_examples[\"attention_mask_rejected\"].append(tokenized_rejected[\"attention_mask\"])\n",
    "\n",
    "    return new_examples\n",
    "\n",
    "tokenized_data = dataset.map(\n",
    "    preprocess_function,\n",
    "    batched=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1991b072",
   "metadata": {},
   "source": [
    "Let's now train a reward model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "85386435",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e2efc26fac145afb1eec4b961b42506",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "model.safetensors:   0%|          | 0.00/338M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at damienbenveniste/mistral-supervised and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForSequenceClassification, AutoTokenizer\n",
    "\n",
    "model_id = 'damienbenveniste/mistral-supervised'\n",
    "\n",
    "reward_model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "f655b40d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "MistralForSequenceClassification(\n",
       "  (model): MistralModel(\n",
       "    (embed_tokens): Embedding(32000, 768)\n",
       "    (layers): ModuleList(\n",
       "      (0-3): 4 x MistralDecoderLayer(\n",
       "        (self_attn): MistralAttention(\n",
       "          (q_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "          (k_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (v_proj): Linear(in_features=768, out_features=384, bias=False)\n",
       "          (o_proj): Linear(in_features=768, out_features=768, bias=False)\n",
       "        )\n",
       "        (mlp): MistralMLP(\n",
       "          (gate_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (up_proj): Linear(in_features=768, out_features=3072, bias=False)\n",
       "          (down_proj): Linear(in_features=3072, out_features=768, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): MistralRMSNorm((768,), eps=1e-06)\n",
       "        (post_attention_layernorm): MistralRMSNorm((768,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): MistralRMSNorm((768,), eps=1e-06)\n",
       "    (rotary_emb): MistralRotaryEmbedding()\n",
       "  )\n",
       "  (score): Linear(in_features=768, out_features=1, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reward_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "8bbefc2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "reward_model.config.pad_token_id = tokenizer.pad_token_id"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "54a98949",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/damienbenveniste/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/torch/utils/data/dataloader.py:683: UserWarning: 'pin_memory' argument is set as true but not supported on MPS now, then device pinned memory won't be used.\n",
      "  warnings.warn(warn_msg)\n",
      "You're using a LlamaTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 06:58, Epoch 1/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=125, training_loss=0.70623828125, metrics={'train_runtime': 421.8177, 'train_samples_per_second': 2.371, 'train_steps_per_second': 0.296, 'total_flos': 0.0, 'train_loss': 0.70623828125, 'epoch': 1.0})"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from trl import RewardTrainer, RewardConfig\n",
    "\n",
    "reward_config = RewardConfig(\n",
    "    output_dir=\"mistral-reward\",\n",
    "    num_train_epochs=1,\n",
    "    push_to_hub=True,\n",
    "    report_to=\"none\",\n",
    ")\n",
    "\n",
    "trainer = RewardTrainer(\n",
    "    model=reward_model,\n",
    "    args=reward_config,\n",
    "    train_dataset=tokenized_data,\n",
    "    processing_class=tokenizer\n",
    ")\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b22a726",
   "metadata": {},
   "source": [
    "And we can push to the hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "8f1297dc",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "No files have been modified since last commit. Skipping to prevent empty commit.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "CommitInfo(commit_url='https://huggingface.co/damienbenveniste/mistral-reward/commit/acd46cb92e3b287960434cb1cae800134136f65a', commit_message='End of training', commit_description='', oid='acd46cb92e3b287960434cb1cae800134136f65a', pr_url=None, repo_url=RepoUrl('https://huggingface.co/damienbenveniste/mistral-reward', endpoint='https://huggingface.co', repo_type='model', repo_id='damienbenveniste/mistral-reward'), pr_revision=None, pr_num=None)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1524441e",
   "metadata": {},
   "source": [
    "## Step 3: PPO Training \n",
    "\n",
    "With our reward model trained, we can now use PPO to optimize our SFT model. The model will generate responses and receive reward scores, learning to produce higher-quality outputs over time. For PPO training, we load a different dataset - the last 1000 examples from Alpaca. We'll use these as prompts for the model to generate responses."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8ccfc785",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"tatsu-lab/alpaca\", split=\"train[-1000:]\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06908927",
   "metadata": {},
   "source": [
    "Let's see another example from the dataset to understand the format:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "82956979",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Given a story, (add/edit/compare/remove) an element from it.\n",
      "\n",
      "### Input:\n",
      "Once upon a time there was a little girl who loved to read books.\n",
      "\n",
      "### Response:\n",
      "Once upon a time there was a little girl who loved to read books and play with her pet rabbit.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95d032cf",
   "metadata": {},
   "source": [
    "  This code loads the necessary components for PPO (Proximal Policy Optimization) training from a pre-trained supervised fine-tuned model:\n",
    "\n",
    "  **`model_id`** - Specifies the Hugging Face model identifier for the previously fine-tuned Mistral model that will serve as the base for PPO training.\n",
    "\n",
    "  **`ppo_model`** - Loads the causal language model that will be optimized during PPO training to generate responses aligned with human preferences.\n",
    "\n",
    "  **`value_model`** - Loads a sequence classification model with a single output (scalar value) that estimates the value of generated sequences for the PPO algorithm.\n",
    "\n",
    "  **`tokenizer`** - Loads the tokenizer associated with the model to handle text preprocessing and encoding for both models during training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "8191a856",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of MistralForSequenceClassification were not initialized from the model checkpoint at damienbenveniste/mistral-supervised and are newly initialized: ['score.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_id = 'damienbenveniste/mistral-supervised'\n",
    "\n",
    "ppo_model = AutoModelForCausalLM.from_pretrained(model_id)\n",
    "value_model = AutoModelForSequenceClassification.from_pretrained(model_id, num_labels=1)\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf404f4d",
   "metadata": {},
   "source": [
    "Each text contains the full instruction-response example. For PPO, we'll extract just the instruction part as prompts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "4d1e2597",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Below is an instruction that describes a task, paired with an input that provides further context. Write a response that appropriately completes the request.\n",
      "\n",
      "### Instruction:\n",
      "Given a story, (add/edit/compare/remove) an element from it.\n",
      "\n",
      "### Input:\n",
      "Once upon a time there was a little girl who loved to read books.\n"
     ]
    }
   ],
   "source": [
    "print(dataset['text'][1].split('### Response')[0].strip())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a92c0878",
   "metadata": {},
   "source": [
    " This code prepares the dataset for PPO training by extracting prompts and tokenizing them:\n",
    "\n",
    "  **`tokenize()`** - Function that splits each sample at the \"### Response\" delimiter to extract only the instruction/prompt portion, then tokenizes it and stores both the token IDs\n",
    "  and raw prompt text.\n",
    "\n",
    "  **`tokenized_dataset`** - Applies the tokenization function to each sample in the dataset, processing them individually rather than in batches.\n",
    "\n",
    "  **`set_format()`** - Converts the dataset to PyTorch tensor format, making it compatible with the PPO training pipeline that expects tensor inputs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "a3c1ce3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(sample):\n",
    "    sample[\"input_ids\"] = tokenizer.encode(\n",
    "        sample[\"text\"].split('### Response')[0].strip(), \n",
    "    )\n",
    "    return sample\n",
    "\n",
    "tokenized_dataset = dataset.map(tokenize, batched=False, remove_columns=dataset.column_names)\n",
    "tokenized_dataset.set_format(type=\"torch\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b22e36f6",
   "metadata": {},
   "source": [
    "This code sets up and runs PPO (Proximal Policy Optimization) training to align the language model with human preferences:\n",
    "\n",
    "**`PPOConfig`** - Configuration object that specifies PPO training parameters including output directory and batch sizes for gradient updates.\n",
    "\n",
    "**`PPOTrainer`** - The main PPO training class that orchestrates the reinforcement learning process using the policy model, reward model, value model, and datasets.\n",
    "\n",
    "**`ppo_trainer.train()`** - Executes the PPO training loop where the model generates responses, receives rewards from the reward model, and updates its policy to maximize human \n",
    "preference alignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "78db3887",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===training policy===\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='375' max='375' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [375/375 1:57:09, Epoch 3/3.0]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from trl import PPOConfig, PPOTrainer\n",
    "\n",
    "ppo_config = PPOConfig(\n",
    "    output_dir=\"mistral-ppo\",\n",
    "    mini_batch_size=2,\n",
    "    batch_size=2,\n",
    ")\n",
    "\n",
    "ppo_trainer = PPOTrainer(\n",
    "    model=ppo_model,\n",
    "    args=ppo_config,\n",
    "    train_dataset=tokenized_dataset,\n",
    "    eval_dataset=tokenized_dataset,\n",
    "    processing_class=tokenizer,\n",
    "    reward_model=reward_model,\n",
    "    value_model=value_model,  \n",
    "    ref_model=None \n",
    ")\n",
    "\n",
    "ppo_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "93670aee",
   "metadata": {},
   "outputs": [
    {
     "ename": "HfHubHTTPError",
     "evalue": "401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-683ccb2b-50472afb5dc3d15b3761d92c;10f79979-e5e9-489c-89c3-efa3195654d6)\n\nInvalid credentials in Authorization header",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mHTTPError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:409\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    408\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m409\u001b[39m     \u001b[43mresponse\u001b[49m\u001b[43m.\u001b[49m\u001b[43mraise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    410\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m e:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/requests/models.py:1024\u001b[39m, in \u001b[36mResponse.raise_for_status\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m   1023\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m http_error_msg:\n\u001b[32m-> \u001b[39m\u001b[32m1024\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m HTTPError(http_error_msg, response=\u001b[38;5;28mself\u001b[39m)\n",
      "\u001b[31mHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[31mHfHubHTTPError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[64]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mppo_trainer\u001b[49m\u001b[43m.\u001b[49m\u001b[43mpush_to_hub\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/transformers/trainer.py:4832\u001b[39m, in \u001b[36mTrainer.push_to_hub\u001b[39m\u001b[34m(self, commit_message, blocking, token, revision, **kwargs)\u001b[39m\n\u001b[32m   4830\u001b[39m \u001b[38;5;66;03m# In case the user calls this method with args.push_to_hub = False\u001b[39;00m\n\u001b[32m   4831\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.hub_model_id \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m4832\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43minit_hf_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   4834\u001b[39m \u001b[38;5;66;03m# Needs to be executed on all processes for TPU training, but will only save on the processed determined by\u001b[39;00m\n\u001b[32m   4835\u001b[39m \u001b[38;5;66;03m# self.args.should_save.\u001b[39;00m\n\u001b[32m   4836\u001b[39m \u001b[38;5;28mself\u001b[39m.save_model(_internal_call=\u001b[38;5;28;01mTrue\u001b[39;00m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/transformers/trainer.py:4644\u001b[39m, in \u001b[36mTrainer.init_hf_repo\u001b[39m\u001b[34m(self, token)\u001b[39m\n\u001b[32m   4641\u001b[39m     repo_name = \u001b[38;5;28mself\u001b[39m.args.hub_model_id\n\u001b[32m   4643\u001b[39m token = token \u001b[38;5;28;01mif\u001b[39;00m token \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m.args.hub_token\n\u001b[32m-> \u001b[39m\u001b[32m4644\u001b[39m repo_url = \u001b[43mcreate_repo\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrepo_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtoken\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprivate\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43margs\u001b[49m\u001b[43m.\u001b[49m\u001b[43mhub_private_repo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mexist_ok\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m   4645\u001b[39m \u001b[38;5;28mself\u001b[39m.hub_model_id = repo_url.repo_id\n\u001b[32m   4646\u001b[39m \u001b[38;5;28mself\u001b[39m.push_in_progress = \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/huggingface_hub/utils/_validators.py:114\u001b[39m, in \u001b[36mvalidate_hf_hub_args.<locals>._inner_fn\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    111\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m check_use_auth_token:\n\u001b[32m    112\u001b[39m     kwargs = smoothly_deprecate_use_auth_token(fn_name=fn.\u001b[34m__name__\u001b[39m, has_token=has_token, kwargs=kwargs)\n\u001b[32m--> \u001b[39m\u001b[32m114\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfn\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/huggingface_hub/hf_api.py:3718\u001b[39m, in \u001b[36mHfApi.create_repo\u001b[39m\u001b[34m(self, repo_id, token, private, repo_type, exist_ok, resource_group_id, space_sdk, space_hardware, space_storage, space_sleep_time, space_secrets, space_variables)\u001b[39m\n\u001b[32m   3715\u001b[39m     \u001b[38;5;28;01mbreak\u001b[39;00m\n\u001b[32m   3717\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3718\u001b[39m     \u001b[43mhf_raise_for_status\u001b[49m\u001b[43m(\u001b[49m\u001b[43mr\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   3719\u001b[39m \u001b[38;5;28;01mexcept\u001b[39;00m HTTPError \u001b[38;5;28;01mas\u001b[39;00m err:\n\u001b[32m   3720\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m exist_ok \u001b[38;5;129;01mand\u001b[39;00m err.response.status_code == \u001b[32m409\u001b[39m:\n\u001b[32m   3721\u001b[39m         \u001b[38;5;66;03m# Repo already exists and `exist_ok=True`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/Projects/Teaching/The-AiEdge-Code-Examples/myenv/lib/python3.13/site-packages/huggingface_hub/utils/_http.py:482\u001b[39m, in \u001b[36mhf_raise_for_status\u001b[39m\u001b[34m(response, endpoint_name)\u001b[39m\n\u001b[32m    478\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, message, response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n\u001b[32m    480\u001b[39m \u001b[38;5;66;03m# Convert `HTTPError` into a `HfHubHTTPError` to display request information\u001b[39;00m\n\u001b[32m    481\u001b[39m \u001b[38;5;66;03m# as well (request id and/or server error message)\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m482\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m _format(HfHubHTTPError, \u001b[38;5;28mstr\u001b[39m(e), response) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01me\u001b[39;00m\n",
      "\u001b[31mHfHubHTTPError\u001b[39m: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-683ccb2b-50472afb5dc3d15b3761d92c;10f79979-e5e9-489c-89c3-efa3195654d6)\n\nInvalid credentials in Authorization header"
     ]
    }
   ],
   "source": [
    "ppo_trainer.push_to_hub()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afa6f9d2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
